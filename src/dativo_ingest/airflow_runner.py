"""Utilities for generating Airflow DAGs from runner configuration."""

from __future__ import annotations

import json
import re
from dataclasses import dataclass
from datetime import UTC, datetime, timedelta
from pathlib import Path
from typing import Dict, Iterable, List, Tuple

from .config import (
    AirflowOrchestratorConfig,
    RunnerConfig,
    ScheduleConfig,
)
from .logging import setup_logging


@dataclass
class GeneratedDag:
    """Metadata about a generated Airflow DAG file."""

    dag_id: str
    file_path: Path


def _sanitize_identifier(value: str) -> str:
    """Convert arbitrary value to a safe Airflow identifier."""
    normalized = value.lower()
    normalized = re.sub(r"[^a-z0-9]+", "_", normalized)
    normalized = normalized.strip("_")
    return normalized or "dativo_job"


def _build_dag_id(prefix: str, schedule: ScheduleConfig) -> str:
    """Build a DAG ID using prefix and sanitized schedule name."""
    sanitized_name = _sanitize_identifier(schedule.name)
    dag_id = f"{prefix}{sanitized_name}"
    return dag_id


def _compose_default_args(
    airflow_cfg: AirflowOrchestratorConfig,
) -> Dict[str, object]:
    """Compose default_args dict for the DAG."""
    default_args: Dict[str, object] = {"owner": airflow_cfg.dag_owner}
    if airflow_cfg.default_args:
        default_args.update(airflow_cfg.default_args)
    return default_args


def _serialize_default_args(default_args: Dict[str, object]) -> str:
    """Serialize default args as pretty JSON for embedding in generated file."""
    return json.dumps(default_args, indent=4, sort_keys=True)


def _determine_schedule_expression(schedule: ScheduleConfig) -> Tuple[str, bool]:
    """Determine schedule expression and whether it is interval-based."""
    if schedule.cron:
        return schedule.cron, False
    if schedule.interval_seconds:
        return str(int(schedule.interval_seconds)), True
    raise ValueError(
        f"Schedule '{schedule.name}' must define either cron or interval_seconds"
    )


def _resolve_start_date(
    schedule: ScheduleConfig, airflow_cfg: AirflowOrchestratorConfig
) -> datetime:
    """Resolve the start date to embed in the DAG definition."""
    if airflow_cfg.start_date:
        return airflow_cfg.start_date

    # Default: set start date to 24 hours before generation in UTC
    return datetime.now(UTC) - timedelta(days=1)


def _build_dag_tags(
    schedule: ScheduleConfig, airflow_cfg: AirflowOrchestratorConfig
) -> List[str]:
    """Combine global and schedule-level tags."""
    tags = list(airflow_cfg.dag_tags or [])

    if schedule.tags:
        schedule_tags = [f"{key}:{value}" for key, value in schedule.tags.items()]
        tags.extend(schedule_tags)

    # Remove duplicates preserving order
    seen = set()
    unique_tags = []
    for tag in tags:
        if tag not in seen:
            seen.add(tag)
            unique_tags.append(tag)
    return unique_tags


def _render_dag_file_content(
    schedule: ScheduleConfig,
    airflow_cfg: AirflowOrchestratorConfig,
    dag_id: str,
    max_active_runs: int,
) -> str:
    """Render the Python file content for a single Airflow DAG."""
    schedule_expression, is_interval = _determine_schedule_expression(schedule)
    default_args_literal = _serialize_default_args(_compose_default_args(airflow_cfg))
    tags_literal = json.dumps(_build_dag_tags(schedule, airflow_cfg))
    start_date = _resolve_start_date(schedule, airflow_cfg)
    start_date_literal = start_date.replace(microsecond=0).isoformat()

    # Determine schedule code snippet
    if is_interval:
        schedule_snippet = (
            f"schedule_interval=timedelta(seconds={schedule_expression})"
        )
    else:
        schedule_snippet = f'schedule="{schedule_expression}"'

    timezone = schedule.timezone or "UTC"

    bash_command = (
        f"{airflow_cfg.python_interpreter} -m dativo_ingest.cli run "
        f"--config {schedule.config} --mode self_hosted"
    )

    return f"""# Generated by dativo_ingest.airflow_runner
from __future__ import annotations

from datetime import timedelta

import pendulum
from airflow import DAG
from airflow.operators.bash import BashOperator

default_args = {default_args_literal}

tz = pendulum.timezone("{timezone}")

with DAG(
    dag_id="{dag_id}",
    {schedule_snippet},
    start_date=pendulum.parse("{start_date_literal}"),
    catchup={str(airflow_cfg.catchup)},
    default_args=default_args,
    max_active_runs={max_active_runs},
    tags={tags_literal},
    timezone=tz,
) as dag:
    run_job = BashOperator(
        task_id="run_{_sanitize_identifier(schedule.name)}",
        bash_command="{bash_command}",
        append_env=True,
    )
"""


def generate_airflow_dags(runner_config: RunnerConfig) -> Iterable[GeneratedDag]:
    """Generate Airflow DAG files for all enabled schedules and return metadata."""
    logger = setup_logging(level="INFO", redact_secrets=False)
    orchestrator = runner_config.orchestrator
    if orchestrator.airflow is None:
        raise ValueError("Airflow configuration is required for Airflow orchestrator")

    airflow_cfg = orchestrator.airflow
    output_dir = Path(airflow_cfg.dag_output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    generated: List[GeneratedDag] = []

    for schedule in orchestrator.schedules:
        if not schedule.enabled:
            logger.info(
                "Skipping disabled Airflow schedule",
                extra={"schedule_name": schedule.name},
            )
            continue

        dag_id = _build_dag_id(airflow_cfg.dag_file_prefix, schedule)
        max_active_runs = min(
            schedule.max_concurrent_runs or 1, orchestrator.concurrency_per_tenant
        )
        content = _render_dag_file_content(
            schedule=schedule,
            airflow_cfg=airflow_cfg,
            dag_id=dag_id,
            max_active_runs=max_active_runs,
        )

        file_path = output_dir / f"{dag_id}.py"
        file_path.write_text(content, encoding="utf-8")
        logger.info(
            "Generated Airflow DAG",
            extra={"dag_id": dag_id, "file_path": str(file_path)},
        )
        generated.append(GeneratedDag(dag_id=dag_id, file_path=file_path))

    return generated


def start_airflow_orchestrator(runner_config: RunnerConfig) -> None:
    """Generate Airflow DAG files and log summary information."""
    logger = setup_logging(level="INFO", redact_secrets=False)
    generated_dags = list(generate_airflow_dags(runner_config))

    if not generated_dags:
        logger.warning(
            "No Airflow DAGs were generated. Ensure schedules are enabled.",
            extra={"event_type": "airflow_no_dags"},
        )
        return

    logger.info(
        "Airflow DAG generation complete",
        extra={
            "event_type": "airflow_dags_generated",
            "dag_count": len(generated_dags),
            "dag_paths": [str(dag.file_path) for dag in generated_dags],
        },
    )
