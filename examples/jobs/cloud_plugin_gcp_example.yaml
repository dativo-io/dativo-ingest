# Example job configuration using custom plugins with GCP Cloud Functions execution
# This demonstrates how to run custom plugins in GCP Cloud Functions for better isolation and scalability

tenant_id: acme
environment: prod

source_connector: custom_api
source_connector_path: /app/connectors/csv.yaml  # Use any connector as base

target_connector: custom_storage
target_connector_path: /app/connectors/s3.yaml  # Use any connector as base

asset: custom_data
asset_path: /app/assets/csv/v1.0/person.yaml  # Use any asset as base

# Source configuration with custom reader and cloud execution
source:
  # Specify custom reader plugin
  # Format: "path/to/module.py:ClassName"
  custom_reader: "/app/plugins/json_api_reader.py:JSONAPIReader"
  
  # Connection details (passed to your custom reader)
  connection:
    base_url: "https://api.example.com/v1"
    timeout: 30
  
  # Credentials (passed to your custom reader)
  credentials:
    token: "${API_TOKEN}"
  
  # Engine options with cloud execution configuration
  engine:
    options:
      page_size: 100
      max_pages: null  # null = no limit
    
    # Cloud execution configuration for GCP Cloud Functions
    cloud_execution:
      enabled: true
      provider: "gcp"
      runtime: "python311"
      memory_mb: 512
      timeout_seconds: 300
      
      # GCP-specific configuration
      gcp:
        # GCP project ID (required)
        project_id: "my-gcp-project"
        
        # Region for Cloud Functions deployment
        region: "us-central1"
        
        # Service account for Cloud Function execution (optional)
        service_account: "plugin-executor@my-gcp-project.iam.gserviceaccount.com"
      
      # Environment variables to pass to Cloud Function
      environment_variables:
        LOG_LEVEL: "INFO"
        CUSTOM_VAR: "value"
  
  # Objects to extract (passed to your custom reader)
  objects: ["users", "orders"]

# Target configuration (standard local execution)
target:
  type: "parquet"
  file_format: "parquet"
  connection:
    s3:
      bucket: "my-data-lake"
      region: "us-east-1"
  engine:
    options:
      compression: "snappy"

# Standard ETL configuration
schema_validation_mode: strict

logging:
  redaction: true
  level: INFO
