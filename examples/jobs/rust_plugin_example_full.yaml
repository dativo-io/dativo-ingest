# Example job configuration using Rust plugins for maximum performance
# Rust plugins provide 10-100x performance improvement for data-intensive operations

tenant_id: acme
environment: prod

source_connector: csv
source_connector_path: /app/connectors/csv.yaml

target_connector: s3
target_connector_path: /app/connectors/s3.yaml

asset: high_performance_data
asset_path: /app/assets/csv/v1.0/person.yaml

# Source configuration with Rust CSV reader
source:
  # Specify Rust plugin (note the .so/.dylib/.dll extension)
  # Format: "path/to/libplugin.so:create_reader"
  custom_reader: "/app/plugins/rust/target/release/libcsv_reader_plugin.so:create_reader"
  
  # Files to process
  files:
    - path: "/data/large_dataset.csv"
      id: "dataset_1"
    - path: "/data/another_dataset.csv"
      id: "dataset_2"
  
  # Engine options (passed to Rust plugin)
  engine:
    options:
      # Larger batch sizes work better with Rust
      batch_size: 50000
      # Delimiter
      delimiter: ","

# Target configuration with Rust Parquet writer
target:
  # Specify Rust plugin
  # Format: "path/to/libplugin.so:create_writer"
  custom_writer: "/app/plugins/rust/target/release/libparquet_writer_plugin.so:create_writer"
  
  # Connection details (passed to Rust plugin)
  connection:
    s3:
      bucket: "my-data-lake"
      region: "us-east-1"
      access_key: "${AWS_ACCESS_KEY_ID}"
      secret_key: "${AWS_SECRET_ACCESS_KEY}"
  
  # Engine options (passed to Rust plugin)
  engine:
    options:
      # Compression: snappy (fast), gzip (balanced), zstd (best compression)
      compression: "zstd"
      # Row group size for Parquet (larger = better compression, more memory)
      row_group_size: 500000

# Standard ETL configuration
schema_validation_mode: strict

logging:
  redaction: true
  level: INFO

# Optional: Retry configuration
retry_config:
  max_attempts: 3
  initial_delay_seconds: 5

