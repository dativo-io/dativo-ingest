╔══════════════════════════════════════════════════════════════════════════════╗
║                                                                              ║
║                   DATA FLOW: READER → WRITER ARCHITECTURE                    ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝

┌──────────────────────────────────────────────────────────────────────────────┐
│ KEY CONCEPT: Python Iterator-Based Streaming                                 │
└──────────────────────────────────────────────────────────────────────────────┘

  ✓ Data flows through memory in BATCHES (no large intermediate buffers)
  ✓ Readers YIELD batches via Python iterators
  ✓ Writers CONSUME batches one at a time
  ✓ Natural backpressure: writer controls reader pace

┌──────────────────────────────────────────────────────────────────────────────┐
│ SIMPLE FLOW                                                                   │
└──────────────────────────────────────────────────────────────────────────────┘

  ┌─────────────┐     Iterator[List[Dict]]     ┌──────────────┐
  │   READER    │ ──────────────────────────▶  │  Main Loop   │
  │             │    yield batch_records        │  (cli.py)    │
  │  • CSV      │                               │              │
  │  • Database │                               │  for batch   │
  │  • API      │                               │  in reader:  │
  │  • Airbyte  │                               │    validate  │
  │  • Rust     │                               │    write     │
  └─────────────┘                               └───────┬──────┘
                                                        │
                                         List[Dict]     │
                                                        ▼
                                               ┌──────────────┐
                                               │   WRITER     │
                                               │              │
                                               │  • Parquet   │
                                               │  • Custom    │
                                               │  • Rust      │
                                               └──────────────┘

┌──────────────────────────────────────────────────────────────────────────────┐
│ DETAILED FLOW WITH VALIDATION                                                │
└──────────────────────────────────────────────────────────────────────────────┘

  Source → Reader → Iterator → [Main Loop] → Validator → Writer → Target
    ↓        ↓         ↓            ↓            ↓          ↓        ↓
   API      Parse    Batch       Iterate     Validate   Write     S3/
   CSV      Data     10K recs    Batches     Schema     Parquet   Iceberg
   DB       Dict     Yield       Control     Filter     File      MinIO

┌──────────────────────────────────────────────────────────────────────────────┐
│ AIRBYTE READER FLOW                                                          │
└──────────────────────────────────────────────────────────────────────────────┘

  ┌──────────────────────────────────────────────────────────────────────┐
  │  1. Python starts Airbyte Docker container                           │
  │     $ docker run airbyte/source-stripe:2.1.5 read --config ...       │
  └──────────────────┬───────────────────────────────────────────────────┘
                     │
         ┌───────────▼──────────┐
         │  Docker Container    │
         │  ┌────────────────┐  │
         │  │ Airbyte Stripe │  │  ──▶  Stripe API (HTTPS calls)
         │  └────────┬───────┘  │
         │           │          │
         │      stdout (JSON)   │
         └───────────┼──────────┘
                     │
         {"type":"RECORD","record":{"id":1,...}}  ← Airbyte Protocol
         {"type":"RECORD","record":{"id":2,...}}
         {"type":"STATE","state":{...}}
                     │
  ┌──────────────────▼───────────────────────────────────────────────────┐
  │  2. Python reads stdout, parses JSON, batches records                │
  │                                                                       │
  │     batch = []                                                        │
  │     for line in process.stdout:                                       │
  │         msg = json.loads(line)                                        │
  │         if msg['type'] == 'RECORD':                                   │
  │             batch.append(msg['record'])                               │
  │         if len(batch) >= 10000:                                       │
  │             yield batch  # ◀── Returns control to main loop           │
  │             batch = []                                                │
  └───────────────────┬───────────────────────────────────────────────────┘
                      │
                      │ Iterator yields: [{"id":1,...}, {"id":2,...}, ...]
                      │
  ┌───────────────────▼───────────────────────────────────────────────────┐
  │  3. Main loop receives batch, validates, writes                       │
  └───────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────────────┐
│ RUST WRITER FLOW                                                             │
└──────────────────────────────────────────────────────────────────────────────┘

  ┌──────────────────────────────────────────────────────────────────────┐
  │  1. Python loads Rust shared library (.so/.dll)                      │
  │                                                                       │
  │     import ctypes                                                     │
  │     lib = ctypes.CDLL("libparquet_writer.so")                        │
  │     writer_ptr = lib.create_writer(config_json)                      │
  └───────────────────┬──────────────────────────────────────────────────┘
                      │
                      │ Memory: Rust writer instance created
                      │
  ┌───────────────────▼──────────────────────────────────────────────────┐
  │  2. Python receives batch, converts to JSON, calls Rust via FFI      │
  │                                                                       │
  │     def write_batch(self, records: List[Dict], file_counter):        │
  │         # Serialize to JSON                                          │
  │         json_str = json.dumps(records).encode('utf-8')               │
  │                                                                       │
  │         # Call Rust via FFI                                          │
  │         result_ptr = lib.write_batch(writer_ptr, json_str)           │
  │                                                                       │
  │         # Get result from Rust                                       │
  │         result_json = ctypes.string_at(result_ptr).decode('utf-8')  │
  │         lib.free_string(result_ptr)                                  │
  │                                                                       │
  │         return json.loads(result_json)                               │
  └───────────────────┬──────────────────────────────────────────────────┘
                      │
                      │ FFI boundary (Python ⇄ Rust)
                      │ Data: JSON string
                      │
  ┌───────────────────▼──────────────────────────────────────────────────┐
  │  3. Rust receives JSON, writes Parquet, returns metadata             │
  │                                                                       │
  │     #[no_mangle]                                                      │
  │     pub extern "C" fn write_batch(                                    │
  │         writer_ptr: *mut Writer,                                      │
  │         records_json: *const c_char                                   │
  │     ) -> *const c_char {                                              │
  │         // Parse JSON to Rust structs                                 │
  │         let records: Vec<Record> = parse_json(records_json);          │
  │                                                                       │
  │         // Write Parquet (10-100x faster than Python!)               │
  │         let file = write_parquet(&records, "output.parquet");        │
  │                                                                       │
  │         // Return metadata as JSON                                    │
  │         return to_json(&file_metadata);                               │
  │     }                                                                  │
  └───────────────────┬──────────────────────────────────────────────────┘
                      │
                      │ Returns: JSON string with file metadata
                      │
  ┌───────────────────▼──────────────────────────────────────────────────┐
  │  4. Python receives metadata, continues to next batch                │
  └───────────────────────────────────────────────────────────────────────┘

┌──────────────────────────────────────────────────────────────────────────────┐
│ AIRBYTE READER + RUST WRITER COMBINATION                                     │
└──────────────────────────────────────────────────────────────────────────────┘

  Stripe API  →  Airbyte Container  →  Python Iterator  →  Rust Writer  →  S3
      ↓               ↓                      ↓                  ↓            ↓
   REST calls    Docker stdout         Batch control      Fast Parquet   Files
   JSON data     JSON lines            Memory mgmt        encoding        Upload

  Flow:
    1. Airbyte calls Stripe API → gets JSON response
    2. Airbyte outputs JSON lines to stdout
    3. Python reads stdout, parses, batches records
    4. Python yields batch to main loop
    5. Main loop validates batch
    6. Python calls Rust writer with batch (JSON over FFI)
    7. Rust writes Parquet file (fast!)
    8. Rust returns file metadata (JSON)
    9. Python continues to next batch

  Performance:
    • Stripe API: 5K records/sec (rate limited)
    • Rust Writer: 100K records/sec (no bottleneck!)
    • Bottleneck: API rate limits, not writer speed

┌──────────────────────────────────────────────────────────────────────────────┐
│ MEMORY EFFICIENCY                                                            │
└──────────────────────────────────────────────────────────────────────────────┘

  Traditional (load all data):
    Dataset: 1,000,000 records × 1 KB = 1 GB memory ❌

  Streaming (iterator-based):
    Batch: 10,000 records × 1 KB = 10 MB memory ✅
    
    Process: Read batch → Validate → Write → Repeat
    Peak memory: ~50 MB (includes validation + writing buffers)
    
    Result: 20x less memory for unlimited dataset size!

┌──────────────────────────────────────────────────────────────────────────────┐
│ CODE EXAMPLE: MAIN LOOP                                                      │
└──────────────────────────────────────────────────────────────────────────────┘

  # From cli.py:execute_job()
  
  # Setup
  extractor = create_extractor(job_config)  # Could be Airbyte, CSV, etc.
  validator = SchemaValidator(asset_definition)
  writer = create_writer(job_config)        # Could be Rust, Parquet, etc.
  
  # Process data in streaming fashion
  for batch_records in extractor.extract(state_manager):
      # batch_records: List[Dict[str, Any]] - typically 10K records
      
      # Validate against schema
      valid_records, errors = validator.validate_batch(batch_records)
      
      # Write to target (Parquet, custom format, etc.)
      if valid_records:
          file_metadata = writer.write_batch(valid_records, file_counter)
          all_files.extend(file_metadata)
          file_counter += len(file_metadata)
  
  # Commit all files
  committer.commit_files(all_files)

┌──────────────────────────────────────────────────────────────────────────────┐
│ KEY BENEFITS                                                                 │
└──────────────────────────────────────────────────────────────────────────────┘

  ✓ Memory Efficient
    → Process billions of records with constant memory (~50 MB)
    
  ✓ Universal Interface
    → Any reader works with any writer (Python, Airbyte, Rust)
    
  ✓ Natural Backpressure
    → Writer controls reader pace (no overwhelming buffers)
    
  ✓ Language Interop
    → Mix Python (orchestration) + Rust (performance) seamlessly
    
  ✓ No Intermediate Storage
    → Data flows through memory only (no temp files)
    
  ✓ Composable
    → Easy to add validation, transformation, filtering in pipeline

┌──────────────────────────────────────────────────────────────────────────────┐
│ PERFORMANCE COMPARISON                                                       │
└──────────────────────────────────────────────────────────────────────────────┘

  Python CSV → Python Parquet:      10,000 records/sec
  Airbyte API → Python Parquet:      5,000 records/sec (API limited)
  Python CSV → Rust Parquet:       100,000 records/sec (10x faster!)
  Airbyte API → Rust Parquet:        5,000 records/sec (API limited, but
                                                         writer never bottlenecks)

  Memory usage (all scenarios):     30-100 MB peak

┌──────────────────────────────────────────────────────────────────────────────┐
│ RELATED DOCUMENTATION                                                        │
└──────────────────────────────────────────────────────────────────────────────┘

  Detailed Explanation:  DATA_FLOW_ARCHITECTURE.md
  Custom Plugins Guide:  docs/CUSTOM_PLUGINS.md
  Execution Flow:        docs/INGESTION_EXECUTION.md
  Performance Testing:   TESTING_PLAYBOOK.md (Test Case 11)

╔══════════════════════════════════════════════════════════════════════════════╗
║                                                                              ║
║  SUMMARY: Data streams through Python iterators, enabling memory-efficient  ║
║           processing with any combination of readers and writers             ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝
