# Iceberg connector - target only
name: iceberg
type: iceberg
roles: [target]  # Target only - data lake format
description: "Iceberg table - writes Parquet files to S3 (catalog optional)"
default_engine:
  type: native
  options:
    native:
      library: "pyiceberg"
      library_version: "0.6.0"
      batch_size_rows: 50000
      max_file_size_mb: 200
      compression: "snappy"
connection_template:
  # Nessie catalog (optional - omit for S3-only writes)
  # nessie:
  #   uri: "${NESSIE_URI}"  # Tenant provides this
  #   ref: "main"
  #   default_branch: "main"
  s3:
    endpoint: "${S3_ENDPOINT}"
    access_key_id: "${AWS_ACCESS_KEY_ID}"
    secret_access_key: "${AWS_SECRET_ACCESS_KEY}"
    region: "${AWS_REGION}"
    path_style_access: false
# Catalog is optional - set to null or omit to write Parquet files directly to S3
# catalog: nessie  # Optional - can be set per job or omitted
file_format: parquet
partitioning_default: [ingest_date]
# Markdown-KV support:
# Markdown-KV format can be stored in Iceberg tables using three options:
# 
# Option 1: Store as STRING column in Parquet (mode: "string")
#   - Markdown-KV content stored as STRING type in Parquet files
#   - Use when you need to preserve original format and query by document
#   - Example: target.markdown_kv_storage.mode: "string"
#
# Option 3: Parse and store as structured data (mode: "structured")
#   - Parse Markdown-KV into structured format before storage
#   - Patterns:
#     * "row_per_kv": One row per key-value pair (query-friendly, great for filtering)
#     * "document_level": One row per document with nested sections (compact)
#     * "hybrid": Both patterns (requires dual-write or two asset definitions)
#   - Example: target.markdown_kv_storage.mode: "structured", structured_pattern: "row_per_kv"

